{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## OpenAI API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from openai import OpenAI\n",
    "import dotenv\n",
    "import os\n",
    "\n",
    "dotenv.load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The chemical formula for water is H‚ÇÇO. This indicates that each molecule of water is composed of two hydrogen atoms (H) and one oxygen atom (O).\n"
     ]
    }
   ],
   "source": [
    "client = OpenAI(api_key=os.getenv(\"OPENAI_API_KEY\"))\n",
    "\n",
    "prompt = \"What is the chemical formula for water?\"\n",
    "\n",
    "response = client.chat.completions.create(\n",
    "    model=\"gpt-4o\",\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": [\n",
    "                {\n",
    "                    \"type\": \"text\",\n",
    "                    \"text\": prompt\n",
    "                }\n",
    "            ]\n",
    "        },\n",
    "    ],\n",
    "\n",
    "    # Optional parameters\n",
    "    temperature=0.3,\n",
    "    max_tokens=1024,\n",
    "    top_p=1,\n",
    "    frequency_penalty=0.1,\n",
    "    presence_penalty=0.1,\n",
    ")\n",
    "\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GeminiAPI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The chemical formula of glucose is **C‚ÇÜH‚ÇÅ‚ÇÇO‚ÇÜ**. \n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import google.generativeai as genai\n",
    "import dotenv\n",
    "\n",
    "\n",
    "dotenv.load_dotenv()\n",
    "\n",
    "genai.configure(api_key=os.getenv(\"GOOGLE_API_KEY\"))\n",
    "\n",
    "# Set up the model\n",
    "generation_config = {\n",
    "    \"temperature\": 0.2,\n",
    "    \"top_p\": 0.8,\n",
    "    \"top_k\": 64,\n",
    "    \"max_output_tokens\": 8192,\n",
    "}\n",
    "\n",
    "model = genai.GenerativeModel(\n",
    "    model_name=\"gemini-1.5-pro\",\n",
    "    generation_config=generation_config,\n",
    ")\n",
    "\n",
    "response = model.generate_content(\"What is the chemical formula of glucose?\")\n",
    "\n",
    "try:\n",
    "    print(response.text)\n",
    "except Exception as e:\n",
    "    print(\"Exception:\\n\", e, \"\\n\")\n",
    "    print(\"Response:\\n\", response.candidates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here are the food items in the fridge and their quantities, to the best of my ability to determine from the image:\n",
      "\n",
      "Top Shelf:\n",
      "\n",
      "* Avocados: 4-5 (partially obscured)\n",
      "* Green Onions: Large bunch\n",
      "* Mushrooms: 6-7\n",
      "* Fresh Herbs: 1 bunch\n",
      "* Salmon Fillet: 1 \n",
      "\n",
      "Bottom Shelf:\n",
      "\n",
      "* Salad Greens: 1 container\n",
      "* Edible Flowers: 6-8\n",
      "* Apples: 3\n",
      "* Oranges: 2\n",
      "* Lemon: 1\n",
      "* Kiwi Fruit: 2 \n",
      "\n",
      "Please note:  It's difficult to be exact with quantities for some items as they are partially hidden. \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Chat with files\n",
    "\n",
    "generation_config = {\n",
    "    \"temperature\": 0.2,\n",
    "    \"top_p\": 0.8,\n",
    "    \"top_k\": 64,\n",
    "    \"max_output_tokens\": 8192,\n",
    "}\n",
    "\n",
    "model = genai.GenerativeModel(\n",
    "    model_name=\"gemini-1.5-pro\",\n",
    "    generation_config=generation_config,\n",
    ")\n",
    "\n",
    "prompt_parts = [\n",
    "    genai.upload_file(\"fridge_food.jpg\"),\n",
    "    \"List the food items in the fridge and their quantities.\"\n",
    "]\n",
    "\n",
    "response = model.generate_content(prompt_parts)\n",
    "\n",
    "try:\n",
    "    print(response.text)\n",
    "except Exception as e:\n",
    "    print(\"Exception:\\n\", e, \"\\n\")\n",
    "    print(\"Response:\\n\", response.candidates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"Les grands mod√®les de langage sont g√©niaux !\" \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Chat history\n",
    "\n",
    "model = genai.GenerativeModel(\"gemini-1.5-flash\")\n",
    "\n",
    "chat_history = [\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"parts\": [\"Hi!\"]\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"model\",\n",
    "        \"parts\": [\"Hi there! How can I help you today?\"],\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"parts\": [\"Translate 'Large Language Models are awesome!' to French.\"],\n",
    "    }\n",
    "]\n",
    "\n",
    "response = model.generate_content(chat_history)\n",
    "\n",
    "try:\n",
    "    print(response.text)\n",
    "except Exception as e:\n",
    "    print(\"Exception:\\n\", e, \"\\n\")\n",
    "    print(\"Response:\\n\", response.candidates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "That's awesome! Pizza is a classic for a reason. What's your favorite kind of pizza?  üçï \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Chat history auto\n",
    "\n",
    "model = genai.GenerativeModel(\"gemini-1.5-flash\", generation_config={\"temperature\": 0.3})\n",
    "chat = model.start_chat(history=[])\n",
    "     \n",
    "prompt_parts = [\"My favourite food is pizza.\"]\n",
    "response = chat.send_message(prompt_parts)\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "As an AI, I don't have access to your personal information, including your favorite food.  You told me your favorite food is pizza, though!  üòä  \n",
      "\n",
      "Is there anything else you'd like to tell me about your favorite food?  Maybe what kind of pizza you like best, or what you like about it? \n",
      "\n"
     ]
    }
   ],
   "source": [
    "prompt_parts = [\"What is my favourite food?\"]\n",
    "response = chat.send_message(prompt_parts)\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[parts {\n",
       "   text: \"My favourite food is pizza.\"\n",
       " }\n",
       " role: \"user\",\n",
       " parts {\n",
       "   text: \"That\\'s awesome! Pizza is a classic for a reason. What\\'s your favorite kind of pizza?  üçï \\n\"\n",
       " }\n",
       " role: \"model\",\n",
       " parts {\n",
       "   text: \"What is my favourite food?\"\n",
       " }\n",
       " role: \"user\",\n",
       " parts {\n",
       "   text: \"As an AI, I don\\'t have access to your personal information, including your favorite food.  You told me your favorite food is pizza, though!  üòä  \\n\\nIs there anything else you\\'d like to tell me about your favorite food?  Maybe what kind of pizza you like best, or what you like about it? \\n\"\n",
       " }\n",
       " role: \"model\"]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat.history"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
